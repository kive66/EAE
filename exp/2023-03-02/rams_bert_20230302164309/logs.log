2023-03-02 16:43:10,749 - config.py - 111 - INFO - ******HYPER-PARAMETERS******
2023-03-02 16:43:10,749 - config.py - 113 - INFO - exp_purpose: decode
2023-03-02 16:43:10,749 - config.py - 113 - INFO - model: bert
2023-03-02 16:43:10,749 - config.py - 113 - INFO - dataset: rams
2023-03-02 16:43:10,749 - config.py - 113 - INFO - train_path: data/rams/decoder/train.json
2023-03-02 16:43:10,749 - config.py - 113 - INFO - test_path: data/rams/decoder/test.json
2023-03-02 16:43:10,749 - config.py - 113 - INFO - exp_path: ./exp/
2023-03-02 16:43:10,749 - config.py - 113 - INFO - save_path: exp/2023-03-02/rams_bert_20230302164309/best.pth
2023-03-02 16:43:10,749 - config.py - 113 - INFO - project_path: ./
2023-03-02 16:43:10,749 - config.py - 113 - INFO - save_scriptList: ['utils', 'models', 'trainer', 'configs']
2023-03-02 16:43:10,749 - config.py - 113 - INFO - do_train: True
2023-03-02 16:43:10,749 - config.py - 113 - INFO - do_test: False
2023-03-02 16:43:10,749 - config.py - 113 - INFO - require_improvement: 200000000
2023-03-02 16:43:10,749 - config.py - 113 - INFO - num_epochs: 20
2023-03-02 16:43:10,749 - config.py - 113 - INFO - batch_size: 4
2023-03-02 16:43:10,749 - config.py - 113 - INFO - test_batch_size: 4
2023-03-02 16:43:10,749 - config.py - 113 - INFO - max_seq_len: 512
2023-03-02 16:43:10,749 - config.py - 113 - INFO - eval_step: 10
2023-03-02 16:43:10,749 - config.py - 113 - INFO - log_step: 100
2023-03-02 16:43:10,749 - config.py - 113 - INFO - pretrain_path: bert-base-uncased
2023-03-02 16:43:10,749 - config.py - 113 - INFO - hidden_size: 768
2023-03-02 16:43:10,749 - config.py - 113 - INFO - logger: <Logger 20230302164309 (DEBUG)>
2023-03-02 16:43:10,750 - config.py - 113 - INFO - tokenizer: BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})
2023-03-02 16:43:10,750 - config.py - 113 - INFO - device: cuda
2023-03-02 16:43:10,750 - config.py - 113 - INFO - basic_learning_rate: 0.0001
2023-03-02 16:43:10,750 - config.py - 113 - INFO - encoder_learning_rate: 3e-05
2023-03-02 16:43:10,750 - config.py - 113 - INFO - rate_warmup_steps: 0.1
2023-03-02 16:43:10,750 - config.py - 113 - INFO - shuffle: True
2023-03-02 16:43:10,750 - config.py - 113 - INFO - drop_last: True
2023-03-02 16:43:10,750 - config.py - 113 - INFO - num_workers: 4
2023-03-02 16:43:10,750 - config.py - 113 - INFO - startTime: 20230302164309
2023-03-02 16:43:10,750 - config.py - 113 - INFO - dev_path: data/rams/decoder/dev.json
2023-03-02 16:43:10,750 - config.py - 113 - INFO - path: exp/2023-03-02/rams_bert_20230302164309
2023-03-02 16:43:10,750 - config.py - 113 - INFO - log_path: exp/2023-03-02/rams_bert_20230302164309/logs.log
2023-03-02 16:43:10,750 - config.py - 113 - INFO - tensorBoard_path: exp/2023-03-02/rams_bert_20230302164309/tensorboard
2023-03-02 16:43:10,750 - config.py - 113 - INFO - script_path: exp/2023-03-02/rams_bert_20230302164309/script
2023-03-02 16:43:10,750 - config.py - 113 - INFO - threshold: 0.4
2023-03-02 16:43:10,750 - config.py - 113 - INFO - max_desc_seq_len: 512
2023-03-02 16:43:10,750 - config.py - 113 - INFO - drop_rate: 0.5
2023-03-02 16:43:10,750 - config.py - 113 - INFO - max_role_num: 5
2023-03-02 16:43:10,750 - config.py - 113 - INFO - event_path: data/rams/event_role_multiplicities.txt
2023-03-02 16:43:10,750 - config.py - 113 - INFO - tbWriter: <torch.utils.tensorboard.writer.SummaryWriter object at 0x7f7415f7da00>
2023-03-02 16:43:10,750 - config.py - 114 - INFO - ****************************
2023-03-02 16:43:10,757 - config.py - 111 - INFO - ******HYPER-PARAMETERS******
2023-03-02 16:43:10,757 - config.py - 113 - INFO - exp_purpose: decode
2023-03-02 16:43:10,757 - config.py - 113 - INFO - model: bert
2023-03-02 16:43:10,757 - config.py - 113 - INFO - dataset: rams
2023-03-02 16:43:10,757 - config.py - 113 - INFO - train_path: data/rams/decoder/train.json
2023-03-02 16:43:10,758 - config.py - 113 - INFO - test_path: data/rams/decoder/test.json
2023-03-02 16:43:10,758 - config.py - 113 - INFO - exp_path: ./exp/
2023-03-02 16:43:10,758 - config.py - 113 - INFO - save_path: exp/2023-03-02/rams_bert_20230302164309/best.pth
2023-03-02 16:43:10,758 - config.py - 113 - INFO - project_path: ./
2023-03-02 16:43:10,758 - config.py - 113 - INFO - save_scriptList: ['utils', 'models', 'trainer', 'configs']
2023-03-02 16:43:10,758 - config.py - 113 - INFO - do_train: True
2023-03-02 16:43:10,758 - config.py - 113 - INFO - do_test: False
2023-03-02 16:43:10,758 - config.py - 113 - INFO - require_improvement: 200000000
2023-03-02 16:43:10,758 - config.py - 113 - INFO - num_epochs: 20
2023-03-02 16:43:10,758 - config.py - 113 - INFO - batch_size: 4
2023-03-02 16:43:10,758 - config.py - 113 - INFO - test_batch_size: 4
2023-03-02 16:43:10,758 - config.py - 113 - INFO - max_seq_len: 512
2023-03-02 16:43:10,758 - config.py - 113 - INFO - eval_step: 10
2023-03-02 16:43:10,758 - config.py - 113 - INFO - log_step: 100
2023-03-02 16:43:10,758 - config.py - 113 - INFO - pretrain_path: bert-base-uncased
2023-03-02 16:43:10,758 - config.py - 113 - INFO - hidden_size: 768
2023-03-02 16:43:10,758 - config.py - 113 - INFO - logger: <Logger 20230302164309 (DEBUG)>
2023-03-02 16:43:10,758 - config.py - 113 - INFO - tokenizer: BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})
2023-03-02 16:43:10,758 - config.py - 113 - INFO - device: cuda
2023-03-02 16:43:10,758 - config.py - 113 - INFO - basic_learning_rate: 0.0001
2023-03-02 16:43:10,758 - config.py - 113 - INFO - encoder_learning_rate: 3e-05
2023-03-02 16:43:10,758 - config.py - 113 - INFO - rate_warmup_steps: 0.1
2023-03-02 16:43:10,758 - config.py - 113 - INFO - shuffle: True
2023-03-02 16:43:10,758 - config.py - 113 - INFO - drop_last: True
2023-03-02 16:43:10,758 - config.py - 113 - INFO - num_workers: 4
2023-03-02 16:43:10,759 - config.py - 113 - INFO - startTime: 20230302164309
2023-03-02 16:43:10,759 - config.py - 113 - INFO - dev_path: data/rams/decoder/dev.json
2023-03-02 16:43:10,759 - config.py - 113 - INFO - path: exp/2023-03-02/rams_bert_20230302164309
2023-03-02 16:43:10,759 - config.py - 113 - INFO - log_path: exp/2023-03-02/rams_bert_20230302164309/logs.log
2023-03-02 16:43:10,759 - config.py - 113 - INFO - tensorBoard_path: exp/2023-03-02/rams_bert_20230302164309/tensorboard
2023-03-02 16:43:10,759 - config.py - 113 - INFO - script_path: exp/2023-03-02/rams_bert_20230302164309/script
2023-03-02 16:43:10,759 - config.py - 113 - INFO - threshold: 0.4
2023-03-02 16:43:10,759 - config.py - 113 - INFO - max_desc_seq_len: 512
2023-03-02 16:43:10,759 - config.py - 113 - INFO - drop_rate: 0.5
2023-03-02 16:43:10,759 - config.py - 113 - INFO - max_role_num: 5
2023-03-02 16:43:10,759 - config.py - 113 - INFO - event_path: data/rams/event_role_multiplicities.txt
2023-03-02 16:43:10,759 - config.py - 113 - INFO - tbWriter: <torch.utils.tensorboard.writer.SummaryWriter object at 0x7fa35e17b6d0>
2023-03-02 16:43:10,759 - config.py - 114 - INFO - ****************************
2023-03-02 16:43:13,911 - decoder_train.py - 48 - INFO - load train set......
2023-03-02 16:43:13,912 - decoder_train.py - 48 - INFO - load train set......
2023-03-02 16:43:16,053 - decoder_train.py - 54 - INFO - load test set......
2023-03-02 16:43:16,053 - decoder_train.py - 54 - INFO - load test set......
2023-03-02 16:43:16,211 - basic_trainer.py - 179 - INFO - ******************** Epoch: 1/20 ***********************
2023-03-02 16:43:16,211 - basic_trainer.py - 179 - INFO - ******************** Epoch: 1/20 ***********************
2023-03-02 16:43:17,169 - decoder_train.py - 64 - ERROR - Traceback (most recent call last):
  File "decoder_train.py", line 62, in <module>
    trainer.train()
  File "/home/wqw/code/BertSum/trainer/basic_trainer.py", line 183, in train
    loss, model_output = self.one_step(data)
  File "/home/wqw/code/BertSum/trainer/decoder/decoder_trainer.py", line 29, in one_step
    loss, module_output = self.model(token_ids, summar_ids, bertsum_ids, entities_ids, role_ids, token_mask, summar_mask, bertsum_mask, entities_mask, role_ids_mask, role_labels, entity_span, char2token, span2entity)
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 619, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/wqw/code/BertSum/models/decoder/decoder.py", line 27, in forward
    forward_loss, forward_ids = self.role_decoder(self.bert, role_ids, role_labels, summar_embedding, token_embedding, entities_embedding, entity_spans, token_mask, role_ids_mask, char2token, span2entity)
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/wqw/code/BertSum/models/layers/role_decoder.py", line 51, in forward
    role_embedding = bert(role_id, role_mask)[0]
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1019, in forward
    encoder_outputs = self.encoder(
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 609, in forward
    layer_outputs = layer_module(
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 537, in forward
    layer_output = apply_chunking_to_forward(
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/transformers/pytorch_utils.py", line 249, in apply_chunking_to_forward
    return forward_fn(*input_tensors)
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 550, in feed_forward_chunk
    layer_output = self.output(intermediate_output, attention_output)
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 464, in forward
    hidden_states = self.LayerNorm(hidden_states + input_tensor)
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/modules/normalization.py", line 169, in forward
    return F.layer_norm(
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/functional.py", line 2094, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps,
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 22.12 GiB already allocated; 12.81 MiB free; 22.32 GiB reserved in total by PyTorch)

2023-03-02 16:43:17,268 - decoder_train.py - 64 - ERROR - Traceback (most recent call last):
  File "decoder_train.py", line 62, in <module>
    trainer.train()
  File "/home/wqw/code/BertSum/trainer/basic_trainer.py", line 183, in train
    loss, model_output = self.one_step(data)
  File "/home/wqw/code/BertSum/trainer/decoder/decoder_trainer.py", line 29, in one_step
    loss, module_output = self.model(token_ids, summar_ids, bertsum_ids, entities_ids, role_ids, token_mask, summar_mask, bertsum_mask, entities_mask, role_ids_mask, role_labels, entity_span, char2token, span2entity)
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 619, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/wqw/code/BertSum/models/decoder/decoder.py", line 27, in forward
    forward_loss, forward_ids = self.role_decoder(self.bert, role_ids, role_labels, summar_embedding, token_embedding, entities_embedding, entity_spans, token_mask, role_ids_mask, char2token, span2entity)
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/wqw/code/BertSum/models/layers/role_decoder.py", line 51, in forward
    role_embedding = bert(role_id, role_mask)[0]
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1019, in forward
    encoder_outputs = self.encoder(
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 609, in forward
    layer_outputs = layer_module(
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 537, in forward
    layer_output = apply_chunking_to_forward(
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/transformers/pytorch_utils.py", line 249, in apply_chunking_to_forward
    return forward_fn(*input_tensors)
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 550, in feed_forward_chunk
    layer_output = self.output(intermediate_output, attention_output)
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 464, in forward
    hidden_states = self.LayerNorm(hidden_states + input_tensor)
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/modules/normalization.py", line 169, in forward
    return F.layer_norm(
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/functional.py", line 2094, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps,
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 1; 23.70 GiB total capacity; 22.12 GiB already allocated; 12.81 MiB free; 22.32 GiB reserved in total by PyTorch)

