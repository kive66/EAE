2023-02-28 16:16:00,122 - config.py - 111 - INFO - ******HYPER-PARAMETERS******
2023-02-28 16:16:00,122 - config.py - 113 - INFO - exp_purpose: decode
2023-02-28 16:16:00,122 - config.py - 113 - INFO - model: bert
2023-02-28 16:16:00,122 - config.py - 113 - INFO - dataset: rams
2023-02-28 16:16:00,122 - config.py - 113 - INFO - train_path: data/rams/decoder/train.json
2023-02-28 16:16:00,122 - config.py - 113 - INFO - test_path: data/rams/decoder/test.json
2023-02-28 16:16:00,122 - config.py - 113 - INFO - exp_path: ./exp/
2023-02-28 16:16:00,122 - config.py - 113 - INFO - save_path: exp/2023-02-28/rams_bert_20230228161559/best.pth
2023-02-28 16:16:00,123 - config.py - 113 - INFO - project_path: ./
2023-02-28 16:16:00,123 - config.py - 113 - INFO - save_scriptList: ['utils', 'models', 'trainer', 'configs']
2023-02-28 16:16:00,123 - config.py - 113 - INFO - do_train: True
2023-02-28 16:16:00,123 - config.py - 113 - INFO - do_test: False
2023-02-28 16:16:00,123 - config.py - 113 - INFO - require_improvement: 200000000
2023-02-28 16:16:00,123 - config.py - 113 - INFO - num_epochs: 20
2023-02-28 16:16:00,123 - config.py - 113 - INFO - batch_size: 1
2023-02-28 16:16:00,123 - config.py - 113 - INFO - test_batch_size: 1
2023-02-28 16:16:00,123 - config.py - 113 - INFO - max_seq_len: 512
2023-02-28 16:16:00,123 - config.py - 113 - INFO - eval_step: 1200
2023-02-28 16:16:00,123 - config.py - 113 - INFO - log_step: 100
2023-02-28 16:16:00,123 - config.py - 113 - INFO - pretrain_path: bert-base-uncased
2023-02-28 16:16:00,123 - config.py - 113 - INFO - hidden_size: 768
2023-02-28 16:16:00,123 - config.py - 113 - INFO - logger: <Logger 20230228161559 (DEBUG)>
2023-02-28 16:16:00,123 - config.py - 113 - INFO - tokenizer: BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})
2023-02-28 16:16:00,123 - config.py - 113 - INFO - device: cuda
2023-02-28 16:16:00,123 - config.py - 113 - INFO - basic_learning_rate: 0.0001
2023-02-28 16:16:00,123 - config.py - 113 - INFO - encoder_learning_rate: 3e-05
2023-02-28 16:16:00,123 - config.py - 113 - INFO - rate_warmup_steps: 0.1
2023-02-28 16:16:00,123 - config.py - 113 - INFO - shuffle: True
2023-02-28 16:16:00,123 - config.py - 113 - INFO - drop_last: True
2023-02-28 16:16:00,123 - config.py - 113 - INFO - num_workers: 4
2023-02-28 16:16:00,123 - config.py - 113 - INFO - startTime: 20230228161559
2023-02-28 16:16:00,123 - config.py - 113 - INFO - dev_path: data/rams/decoder/dev.json
2023-02-28 16:16:00,123 - config.py - 113 - INFO - path: exp/2023-02-28/rams_bert_20230228161559
2023-02-28 16:16:00,123 - config.py - 113 - INFO - log_path: exp/2023-02-28/rams_bert_20230228161559/logs.log
2023-02-28 16:16:00,124 - config.py - 113 - INFO - tensorBoard_path: exp/2023-02-28/rams_bert_20230228161559/tensorboard
2023-02-28 16:16:00,124 - config.py - 113 - INFO - script_path: exp/2023-02-28/rams_bert_20230228161559/script
2023-02-28 16:16:00,124 - config.py - 113 - INFO - threshold: 0.5
2023-02-28 16:16:00,124 - config.py - 113 - INFO - max_desc_seq_len: 512
2023-02-28 16:16:00,124 - config.py - 113 - INFO - drop_rate: 0.5
2023-02-28 16:16:00,124 - config.py - 113 - INFO - max_role_num: 5
2023-02-28 16:16:00,124 - config.py - 113 - INFO - event_path: data/rams/event_role_multiplicities.txt
2023-02-28 16:16:00,124 - config.py - 113 - INFO - tbWriter: <torch.utils.tensorboard.writer.SummaryWriter object at 0x7f8d06a41ee0>
2023-02-28 16:16:00,124 - config.py - 114 - INFO - ****************************
2023-02-28 16:16:06,787 - decoder_train.py - 34 - INFO - load train set......
2023-02-28 16:16:08,761 - decoder_train.py - 38 - INFO - load test set......
2023-02-28 16:16:09,170 - basic_trainer.py - 179 - INFO - ******************** Epoch: 1/20 ***********************
2023-02-28 16:16:10,065 - basic_trainer.py - 191 - INFO - step: 0/7327, Train loss: 187.39
2023-02-28 16:17:33,639 - basic_trainer.py - 191 - INFO - step: 100/7327, Train loss: 1173.70
2023-02-28 16:18:56,663 - basic_trainer.py - 191 - INFO - step: 200/7327, Train loss: 766.49
2023-02-28 16:20:20,042 - basic_trainer.py - 191 - INFO - step: 300/7327, Train loss: 306.81
2023-02-28 16:21:42,449 - basic_trainer.py - 191 - INFO - step: 400/7327, Train loss: 49.19
2023-02-28 16:23:05,563 - basic_trainer.py - 191 - INFO - step: 500/7327, Train loss: 26.44
2023-02-28 16:24:28,082 - basic_trainer.py - 191 - INFO - step: 600/7327, Train loss: 20.75
2023-02-28 16:25:51,233 - basic_trainer.py - 191 - INFO - step: 700/7327, Train loss: 21.69
2023-02-28 16:27:13,706 - basic_trainer.py - 191 - INFO - step: 800/7327, Train loss: 21.98
2023-02-28 16:28:35,238 - basic_trainer.py - 191 - INFO - step: 900/7327, Train loss: 19.73
2023-02-28 16:29:58,330 - basic_trainer.py - 191 - INFO - step: 1000/7327, Train loss: 20.66
2023-02-28 16:31:20,528 - basic_trainer.py - 191 - INFO - step: 1100/7327, Train loss: 19.85
2023-02-28 16:32:42,557 - basic_trainer.py - 191 - INFO - step: 1200/7327, Train loss: 19.46
2023-02-28 16:36:43,204 - decoder_trainer.py - 74 - INFO - Test loss: 19.00
2023-02-28 16:36:43,205 - decoder_trainer.py - 75 - INFO - ---------------------------------------------------------------------
2023-02-28 16:36:43,205 - decoder_trainer.py - 76 - INFO - Arg      - P:   0.00            , R:   0.00            , F:   0.00
2023-02-28 16:36:43,205 - decoder_trainer.py - 78 - INFO - ---------------------------------------------------------------------
2023-02-28 16:36:43,215 - basic_trainer.py - 234 - INFO - step: 1200/7327, test acc: 0, Time usage: 0:20:34 [*]
2023-02-28 16:38:06,658 - basic_trainer.py - 191 - INFO - step: 1300/7327, Train loss: 17.22
2023-02-28 16:39:29,880 - basic_trainer.py - 191 - INFO - step: 1400/7327, Train loss: 20.28
2023-02-28 16:40:52,350 - basic_trainer.py - 191 - INFO - step: 1500/7327, Train loss: 16.69
2023-02-28 16:42:14,881 - basic_trainer.py - 191 - INFO - step: 1600/7327, Train loss: 18.86
2023-02-28 16:43:37,444 - basic_trainer.py - 191 - INFO - step: 1700/7327, Train loss: 17.49
2023-02-28 16:45:01,815 - basic_trainer.py - 191 - INFO - step: 1800/7327, Train loss: 18.58
2023-02-28 16:46:25,182 - basic_trainer.py - 191 - INFO - step: 1900/7327, Train loss: 19.06
2023-02-28 16:47:48,229 - basic_trainer.py - 191 - INFO - step: 2000/7327, Train loss: 18.79
2023-02-28 16:49:11,313 - basic_trainer.py - 191 - INFO - step: 2100/7327, Train loss: 18.69
2023-02-28 16:50:33,545 - basic_trainer.py - 191 - INFO - step: 2200/7327, Train loss: 17.79
2023-02-28 16:51:54,781 - basic_trainer.py - 191 - INFO - step: 2300/7327, Train loss: 18.42
2023-02-28 16:53:15,891 - basic_trainer.py - 191 - INFO - step: 2400/7327, Train loss: 15.75
2023-02-28 16:57:31,496 - decoder_trainer.py - 74 - INFO - Test loss: 20.40
2023-02-28 16:57:31,496 - decoder_trainer.py - 75 - INFO - ---------------------------------------------------------------------
2023-02-28 16:57:31,496 - decoder_trainer.py - 76 - INFO - Arg      - P:  39.50            , R:   5.68            , F:   9.94
2023-02-28 16:57:31,496 - decoder_trainer.py - 78 - INFO - ---------------------------------------------------------------------
2023-02-28 16:57:31,507 - basic_trainer.py - 234 - INFO - step: 2400/7327, test acc: 0.09935391241923906, Time usage: 0:41:22 [*]
2023-02-28 16:59:04,270 - basic_trainer.py - 191 - INFO - step: 2500/7327, Train loss: 16.12
2023-02-28 17:00:25,975 - basic_trainer.py - 191 - INFO - step: 2600/7327, Train loss: 14.98
2023-02-28 17:01:50,123 - basic_trainer.py - 191 - INFO - step: 2700/7327, Train loss: 18.13
2023-02-28 17:03:14,168 - basic_trainer.py - 191 - INFO - step: 2800/7327, Train loss: 16.69
2023-02-28 17:04:42,602 - basic_trainer.py - 191 - INFO - step: 2900/7327, Train loss: 17.01
2023-02-28 17:06:09,593 - basic_trainer.py - 191 - INFO - step: 3000/7327, Train loss: 17.17
2023-02-28 17:07:34,389 - basic_trainer.py - 191 - INFO - step: 3100/7327, Train loss: 15.75
2023-02-28 17:08:43,018 - decoder_train.py - 46 - ERROR - Traceback (most recent call last):
  File "decoder_train.py", line 44, in <module>
    trainer.train()
  File "/home/wqw/code/BertSum/trainer/basic_trainer.py", line 183, in train
    loss, model_output = self.one_step(data)
  File "/home/wqw/code/BertSum/trainer/decoder/decoder_trainer.py", line 27, in one_step
    loss, module_output = self.model(token_ids, summar_ids, bertsum_ids, entities_ids, role_ids, token_mask, summar_mask, bertsum_mask, entities_mask, role_ids_mask, role_labels, entity_span, char2token)
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 159, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/wqw/code/BertSum/models/decoder/decoder.py", line 27, in forward
    forward_loss, forward_role = self.role_decoder(self.bert, role_ids, role_labels, summar_embedding, token_embedding, entities_embedding, entity_spans, token_mask, role_ids_mask, char2token)
  File "/home/wqw/miniconda3/envs/BertSum/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/wqw/code/BertSum/models/layers/role_decoder.py", line 63, in forward
    pred_arg = self.arg_map(single_word_pred_arg, multi_word_pred_arg, entity_spans, char2token)
  File "/home/wqw/code/BertSum/models/layers/role_decoder.py", line 91, in arg_map
    pred_arg[k] = min((pred_arg[k] + multi_pred)/2, 1.0)
IndexError: index 537 is out of bounds for dimension 0 with size 512

