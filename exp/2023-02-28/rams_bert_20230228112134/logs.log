2023-02-28 11:21:35,540 - config.py - 111 - INFO - ******HYPER-PARAMETERS******
2023-02-28 11:21:35,541 - config.py - 113 - INFO - exp_purpose: decode
2023-02-28 11:21:35,541 - config.py - 113 - INFO - model: bert
2023-02-28 11:21:35,541 - config.py - 113 - INFO - dataset: rams
2023-02-28 11:21:35,541 - config.py - 113 - INFO - train_path: data/rams/decoder/train.json
2023-02-28 11:21:35,541 - config.py - 113 - INFO - test_path: data/rams/decoder/test.json
2023-02-28 11:21:35,541 - config.py - 113 - INFO - exp_path: ./exp/
2023-02-28 11:21:35,541 - config.py - 113 - INFO - save_path: exp/2023-02-28/rams_bert_20230228112134/best.pth
2023-02-28 11:21:35,541 - config.py - 113 - INFO - project_path: ./
2023-02-28 11:21:35,541 - config.py - 113 - INFO - save_scriptList: ['utils', 'models', 'trainer', 'configs']
2023-02-28 11:21:35,541 - config.py - 113 - INFO - do_train: True
2023-02-28 11:21:35,541 - config.py - 113 - INFO - do_test: False
2023-02-28 11:21:35,541 - config.py - 113 - INFO - require_improvement: 200000000
2023-02-28 11:21:35,541 - config.py - 113 - INFO - num_epochs: 20
2023-02-28 11:21:35,541 - config.py - 113 - INFO - batch_size: 2
2023-02-28 11:21:35,541 - config.py - 113 - INFO - test_batch_size: 1
2023-02-28 11:21:35,541 - config.py - 113 - INFO - max_seq_len: 512
2023-02-28 11:21:35,541 - config.py - 113 - INFO - eval_step: 1400
2023-02-28 11:21:35,541 - config.py - 113 - INFO - log_step: 100
2023-02-28 11:21:35,541 - config.py - 113 - INFO - pretrain_path: bert-base-uncased
2023-02-28 11:21:35,541 - config.py - 113 - INFO - hidden_size: 768
2023-02-28 11:21:35,541 - config.py - 113 - INFO - logger: <Logger 20230228112134 (DEBUG)>
2023-02-28 11:21:35,541 - config.py - 113 - INFO - tokenizer: BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})
2023-02-28 11:21:35,542 - config.py - 113 - INFO - device: cuda
2023-02-28 11:21:35,542 - config.py - 113 - INFO - basic_learning_rate: 0.0001
2023-02-28 11:21:35,542 - config.py - 113 - INFO - encoder_learning_rate: 3e-05
2023-02-28 11:21:35,542 - config.py - 113 - INFO - rate_warmup_steps: 0.1
2023-02-28 11:21:35,542 - config.py - 113 - INFO - shuffle: True
2023-02-28 11:21:35,542 - config.py - 113 - INFO - drop_last: True
2023-02-28 11:21:35,542 - config.py - 113 - INFO - num_workers: 4
2023-02-28 11:21:35,542 - config.py - 113 - INFO - startTime: 20230228112134
2023-02-28 11:21:35,542 - config.py - 113 - INFO - dev_path: data/rams/decoder/dev.json
2023-02-28 11:21:35,542 - config.py - 113 - INFO - path: exp/2023-02-28/rams_bert_20230228112134
2023-02-28 11:21:35,542 - config.py - 113 - INFO - log_path: exp/2023-02-28/rams_bert_20230228112134/logs.log
2023-02-28 11:21:35,542 - config.py - 113 - INFO - tensorBoard_path: exp/2023-02-28/rams_bert_20230228112134/tensorboard
2023-02-28 11:21:35,542 - config.py - 113 - INFO - script_path: exp/2023-02-28/rams_bert_20230228112134/script
2023-02-28 11:21:35,542 - config.py - 113 - INFO - threshold: 0.4
2023-02-28 11:21:35,542 - config.py - 113 - INFO - max_desc_seq_len: 512
2023-02-28 11:21:35,542 - config.py - 113 - INFO - drop_rate: 0.5
2023-02-28 11:21:35,542 - config.py - 113 - INFO - max_role_num: 5
2023-02-28 11:21:35,542 - config.py - 113 - INFO - event_path: data/rams/event_role_multiplicities.txt
2023-02-28 11:21:35,542 - config.py - 113 - INFO - tbWriter: <torch.utils.tensorboard.writer.SummaryWriter object at 0x7fe3d332be80>
2023-02-28 11:21:35,542 - config.py - 114 - INFO - ****************************
2023-02-28 11:21:41,932 - decoder_train.py - 35 - INFO - load train set......
2023-02-28 11:21:43,911 - decoder_train.py - 39 - INFO - load test set......
2023-02-28 11:21:44,312 - basic_trainer.py - 179 - INFO - ******************** Epoch: 1/20 ***********************
2023-02-28 11:21:45,264 - basic_trainer.py - 191 - INFO - step: 0/3664, Train loss: 823.47
2023-02-28 11:22:58,773 - basic_trainer.py - 191 - INFO - step: 100/3664, Train loss: 1036.34
2023-02-28 11:24:12,964 - basic_trainer.py - 191 - INFO - step: 200/3664, Train loss: 445.79
2023-02-28 11:25:26,813 - basic_trainer.py - 191 - INFO - step: 300/3664, Train loss: 55.72
2023-02-28 11:26:41,597 - basic_trainer.py - 191 - INFO - step: 400/3664, Train loss: 22.05
2023-02-28 11:27:55,532 - basic_trainer.py - 191 - INFO - step: 500/3664, Train loss: 22.03
2023-02-28 11:29:10,030 - basic_trainer.py - 191 - INFO - step: 600/3664, Train loss: 20.68
2023-02-28 11:30:24,217 - basic_trainer.py - 191 - INFO - step: 700/3664, Train loss: 18.01
2023-02-28 11:31:38,904 - basic_trainer.py - 191 - INFO - step: 800/3664, Train loss: 19.60
2023-02-28 11:32:53,676 - basic_trainer.py - 191 - INFO - step: 900/3664, Train loss: 19.17
2023-02-28 11:34:06,789 - basic_trainer.py - 191 - INFO - step: 1000/3664, Train loss: 17.31
2023-02-28 11:35:21,375 - basic_trainer.py - 191 - INFO - step: 1100/3664, Train loss: 17.89
2023-02-28 11:36:35,556 - basic_trainer.py - 191 - INFO - step: 1200/3664, Train loss: 16.69
2023-02-28 11:37:50,605 - basic_trainer.py - 191 - INFO - step: 1300/3664, Train loss: 18.28
2023-02-28 11:39:05,228 - basic_trainer.py - 191 - INFO - step: 1400/3664, Train loss: 16.69
2023-02-28 11:40:19,249 - basic_trainer.py - 191 - INFO - step: 1500/3664, Train loss: 15.73
2023-02-28 11:41:33,654 - basic_trainer.py - 191 - INFO - step: 1600/3664, Train loss: 16.41
2023-02-28 11:42:47,003 - basic_trainer.py - 191 - INFO - step: 1700/3664, Train loss: 16.19
2023-02-28 11:44:00,993 - basic_trainer.py - 191 - INFO - step: 1800/3664, Train loss: 16.69
2023-02-28 11:45:14,994 - basic_trainer.py - 191 - INFO - step: 1900/3664, Train loss: 14.84
2023-02-28 11:46:28,720 - basic_trainer.py - 191 - INFO - step: 2000/3664, Train loss: 15.58
2023-02-28 11:47:41,273 - basic_trainer.py - 191 - INFO - step: 2100/3664, Train loss: 15.70
2023-02-28 11:48:54,664 - basic_trainer.py - 191 - INFO - step: 2200/3664, Train loss: 17.34
2023-02-28 11:50:08,393 - basic_trainer.py - 191 - INFO - step: 2300/3664, Train loss: 18.26
2023-02-28 11:51:22,023 - basic_trainer.py - 191 - INFO - step: 2400/3664, Train loss: 16.62
2023-02-28 11:52:35,951 - basic_trainer.py - 191 - INFO - step: 2500/3664, Train loss: 14.63
